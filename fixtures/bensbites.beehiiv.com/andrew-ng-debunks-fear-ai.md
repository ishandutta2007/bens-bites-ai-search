# Andrew Ng debunks the fear of AI

In the past few days, the heat between legends of the AI realm has been increasing. The battle is about [the risks of AI](https://www.deeplearning.ai/the-batch/issue-220/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=andrew-ng-debunks-the-fear-of-ai). Andrew Ng, the co-founder of Google Brain and Coursera believes that big tech is pushing the human extinction by AI theme for their benefit and other researchers are adding fuel to the fire.

## What's going on here?

Andrew believes while AI risks exist, fear-mongering causes overblown anxiety. Baseless warnings of AI-driven human extinction could impede AI progress that benefits humanity.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/99a86485-f736-40dd-a03f-ff6f10712dcc/image.png)

## What does this mean?

Sincerity exists, but so do individuals/organizations who have strong financial incentives by overflowing the human extinction and superintelligent AI stories. Here’s a quick list of some legitimate fears and solutions in progress:

1. Hallucination that leads to misinformation, but developers are improving models and adding filters.

2. Easy access to tools for cybercrime: deepfakes, voice clones, malicious bots etc. Governments and companies are diverting resources to red teaming.

3. Copyright disputes for training data breach trust, but companies are finding legal ways to access data.

4. Fears that AI will eliminate human jobs are exaggerated since AI tends to transform jobs rather than replace them outright.

5. AI startup hype may lead to a correction, but the usefulness of generative AI means it's here to stay.

## Why should I care?

While the researchers are debating pro and cons like they are their dorm room, take a step away. Think about who has what incentives. Dig deep to evaluate if the claims hold on basic interrogation.

If you’re a student, a builder, or a developer, focus on finding the legitimate fears and facing them head-on.
