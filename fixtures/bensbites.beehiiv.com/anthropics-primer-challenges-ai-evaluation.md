# Anthropic's primer on challenges in AI evaluation.

The AI safety team at [Anthropic outlines the challenges in AI evaluation](https://www.anthropic.com/index/evaluating-ai-systems?utm_source=bensbites\&utm_medium=referral\&utm_campaign=anthropic-s-primer-on-challenges-in-ai-evaluation) across multiple dimensions like accuracy, bias, and harm. They share what they have learned from extensive experience developing and implementing standardized tests and human evaluations of AI systems.

## What's going on here?

Anthropic explain evaluating AI is complex; even basic benchmarks require extensive engineering and hide pitfalls.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/c98c6b69-e86c-416c-950c-7a9e710abee6/image.png)

## What does this mean?

From multiple-choice tests to human ratings, Anthropic has found every evaluation approach has major limitations.

Small changes can swing scores for multiple choice evals which makes them easy to cheat. More complex evaluations like tests for bias and harm take months to properly implement, and hide their own flaws until you dig deeper.

Real-world human evaluations of harm and social impacts are expensive, subjective and legally risky. Third parties build great frameworks, but adapting our models for them takes non-trivial effort. More collaborating between companies and evaluators would help, but balancing objectivity with expertise is tricky.

## Why should I care?

As AI becomes more capable, having trustworthy ways to evaluate progress and safety will be crucial for developing beneficial systems. However, current evaluation methods are limited, so we can't fully rely on them.

AI developers, policymakers, and companies using AI must recognize evaluations' shortcomings today. More funding and coordination are needed to improve them. With thoughtful research and engineering, we can build the trustworthy, unbiased assessments essential for deploying AI safely at scale..
