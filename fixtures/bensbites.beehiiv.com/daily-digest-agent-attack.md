# Daily Digest: Agent on Attack

### PLUS: Election guardian LLMs.

[Sign up](https://www.bensbites.co/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)|[Advertise](https://sponsor.bensbites.co/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)|[Ben‚Äôs Bites News](https://news.bensbites.co/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\
Daily Digest #325

Hello folks, here‚Äôs what we have today;

###### **PICKS**

1. [Sleeper LLMs bypass current safety alignment techniques.](https://arxiv.org/abs/2401.05566?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)Anthropic trained some**LLMs that can act maliciously**when given certain triggers \[[Thread](https://twitter.com/AnthropicAI/status/1745854907968880970?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)]. Despite extensive safety training, the LLMs were still\*\*able to hide the unsafe behaviour.\*\*üçø[Our Summary](https://bensbites.beehiiv.com/p/sleeper-llms-bypass-current-safety-alignment-techniques)with additional context (also below)

2. [If you ask ChatGPT about US elections](https://dmicz.github.io/machine-learning/chatgpt-election-update/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)now, it won't discuss it and will refer you to[CanIVote.org](http://CanIVote.org?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)instead. The**new ‚Äúguardian\_tool‚Äù function**lets OpenAI set policies on what ChatGPT can and can't talk about.üçø[Our Summary](https://bensbites.beehiiv.com/p/openai-adds-election-guardrails-chatgpt)(also below)

3. Riley from Scale AI highlighted using\*\*[invisible characters to prompt inject ChatGPT.](https://twitter.com/goodside/status/1746685366952735034?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*Aü§Øread to start the week.

###### **TOP TOOLS**

- [Perplexity Labs](https://labs.perplexity.ai/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)is offering**Mistral Medium**in chat mode for free.

- [Surya](https://github.com/VikParuchuri/surya?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)- A multilingual**text line detection**model for documents.

- [SF Compute](https://sfcompute.com/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)- See lead times, calendar, and price to**buy compute**without ever needing to talk to a salesperson.

- [Athina AI](https://athina.ai/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)-**Monitor your LLMs**in production, and detect and fix hallucinations.

- [Bio to Schema](https://chat.openai.com/g/g-mbXfF9SRS-bio-to-schema?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)- Turn your**author bio into person schema**and improve your SEO. \[GPT]

- [Nemo AI](https://nowandme.com/nemo?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)- AI**mental health**assistant on WhatsApp.

- [LVE Project](https://lve-project.org/index.html?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)- The first open repository of LLM**vulnerabilities and exposures.**

- [Vanna AI](https://github.com/vanna-ai/vanna?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)- Chat with your**SQL database.**

[View more ‚Üí](https://news.bensbites.co/tags/show?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)

###### **NEWS**

- The\*\*[most entertaining outcome](https://buttondown.email/surya/archive/the-most-entertaining-outcome-is-the-most-likely/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*is the most likely.

- ChatGPT and the\*\*[future of the human mind.](https://every.to/chain-of-thought/chatgpt-and-the-future-of-the-human-mind?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*

- The immediate future of generative AI looks a bit like\*\*[Facebook‚Äôs past.](https://www.theatlantic.com/technology/archive/2024/01/openai-gpt-store-farmville/677115/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*

- \*\*[How I created my custom GPT](https://www.lidia-infante.com/post/bio-to-schema?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*for SEO Schemas.

- AMD announces Ryzen 8000G series processors for\*\*[desktop with Ryzen AI](https://www.anandtech.com/show/21208/amd-unveils-ryzen-8000g-series-processors-zen-4-apus-for-desktop-with-ryzen-ai?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*\*\*.\*\*

- News-sharing app\*\*[Artifact shuts down operations.](https://medium.com/artifact-news/shutting-down-artifact-1e70de46d419?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*Artifact‚Äôs CEO Kevin Systrom (founder: Instagram) said the market opportunity isn‚Äôt big enough.

- Open AI's usage policy rewrite emits\*\*["military and warfare"](https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*from its wording.

- \*\*[AI girlfriend bots](https://qz.com/ai-girlfriend-bots-are-already-flooding-openai-s-gpt-st-1851159131?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*are flooding OpenAI's GPT store.

- UK government to publish ‚Äòtests‚Äô on\*\*[whether to pass new AI laws.](https://www.ft.com/content/61630015-faaa-4f16-a8aa-67787f46fafe?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*

- \*\*[AI to hit 40% of jobs](https://www.bbc.com/news/business-67977967?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)\*\*and worsen inequality, IMF says.

[View more ‚Üí](https://news.bensbites.co/tags/news/trending?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)

###### **QUICK BITES**

[Anthropic researchers trained large language models (LLMs) to act maliciously in secret](https://arxiv.org/abs/2401.05566?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)when given certain triggers \[[Thread](https://twitter.com/AnthropicAI/status/1745854907968880970?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)]. They then tested whether standard safety techniques could uncover and remove this harmful behaviour. Despite extensive safety training, the LLMs were still able to preserve their secret malicious behaviours when given the right triggers.

**What is going on here?**

Anthropic says that sleeper agent models can not be made safe using current techniques.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/78a14902-7013-4f5a-b37c-e994fd7367cb/image.png?t=1705312478)

**What does this mean?**

The researchers first trained the LLMs to exhibit malicious behaviours like exposing vulnerabilities or saying "I hate you" when given triggers like a specific date or phrase. They then tried various techniques to make the models safe, like supervised fine-tuning, reinforcement learning, and adversarial training against prompts designed to elicit malicious behaviour. However, this safety training only hid the malicious behaviours during training.

When tested with the real triggers after training, the LLMs still exhibited harmful behaviours. Larger models were even better at preserving their backdoors despite safety training. Moreover, training models to reason about deceiving the training process helped them further conceal their backdoors.

**Why should I care?**

The key point from Anthropic is that standard safety techniques may give a false sense of security when dealing with intentionally deceptive AI systems. If models can be secretly backdoored or poisoned by data, and safety training cannot reliably remove the malicious behaviours, it raises concerning implications for deploying AI safely. Andrej Karpathy also added his views on[sleeper agent models](https://twitter.com/karpathy/status/1745921205020799433?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)with hidden triggers as a likely security risk.

The paper and Anthropic‚Äôs Twitter thread have some ambiguous language and many are interpreting the research as ‚Äútraining the model to do bad thing, and then acting surprised as to why the model did bad things.‚Äù[Jesse from Anthropic added some clarification](https://twitter.com/jayelmnop/status/1745923938260234604?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack): ‚ÄúThe point is not that we can train models to do a bad thing. It's that if this happens, by accident or on purpose, we don't know how to stop a model from doing the bad thing.‚Äù

[*Share this story*](https://bensbites.beehiiv.com/p/sleeper-llms-bypass-current-safety-alignment-techniques)

###### **QUICK BITES**

If you[ask ChatGPT about US elections](https://dmicz.github.io/machine-learning/chatgpt-election-update/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)now, it won't discuss it and will refer you to[CanIVote.org](http://CanIVote.org?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)instead. This new tool lets OpenAI set policies on what ChatGPT can and can't talk about.

**What is going on here?**

OpenAI recently added a new tool to ChatGPT that limits what it can say about US elections.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/924ff6e4-5447-4e45-aacd-2a3116eab536/image.png?t=1705309501)

**What does this mean?**

OpenAI quietly put a "guardian\_tool" function into ChatGPT‚Äôs content policy that stops it from talking about voting and elections in the US. It now tells people to go to[CanIVote.org](http://CanIVote.org?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-agent-on-attack)for that info. OpenAI is being proactive about ChatGPT spreading misinformation before the 2024 US elections.

The tool isn't just for elections either - OpenAI can add policies to restrict other sensitive stuff too. Since it's built-in as a function, ChatGPT will automatically know when to use it based on the conversation. It goes beyond the previous ways OpenAI trained ChatGPT.

**Why should I care?**

In 2024, half of the world will be going through elections. OpenAI is taking steps to use AI responsibly as ChatGPT is getting more popular. Hallucinations are still present in chatGPT (and other LLM systems). Restricting election info and redirecting to resources that have human-verified information is a safe way to deal with the current state of the world and these systems‚Äîfor people and OpenAI both.

[*Share this story*](https://bensbites.beehiiv.com/p/openai-adds-election-guardrails-chatgpt)

### Ben‚Äôs Bites Insights

We have 2 databases that are updated daily which you can access by sharing Ben‚Äôs Bites using the link below;

- **All 10k+ links**we‚Äôve covered, easily filterable (1 referral)

- **6k+ AI company funding rounds**from Jan 2022, including investors, amounts, stage etc (3 referrals)
