# Daily Digest: What's possible? Everything.

### PLUS: Ai models are larger, better, and not done yet.

[S](https://bensbites.com?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)[ubscribe](https://bensbites.com?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)|[Ben‚Äôs Bites Pro](https://bensbites.beehiiv.com/upgrade)|[Ben‚Äôs Bites News](https://news.bensbites.co/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\
Daily Digest #349

Want to get in front of 100k AI enthusiasts?[Work with us here](https://grizzlyads.com/store/bens-bites?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)

Hello folks, here‚Äôs what we have today;

###### **PICKS**

1. ICYMI I wrote about a company with[a very ‚Äòout-there‚Äô initiative on getting its employees to use AI](https://bensbites.beehiiv.com/p/ignitetech-using-ai-change-business), ‚Äúif you don‚Äôt engage you will be let go.‚Äù

2. [Google announced a new model.](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)Didn‚Äôt they release one last week? That was Gemini Ultra 1.0. Google‚Äôs moving ahead to 1.5 model announcements and now we got a**peek into Gemini Pro 1.5**. This one has a context window of up to 10M tokens‚ÄîGPT-4 Turbo has 128k.üçø[Our Summary](https://bensbites.beehiiv.com/p/gemini-pro-15-google-10m-content-window)(also below)

3. [OpenAI's mogging everyone in the AI space again.](https://openai.com/sora?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)**Sora,**a new AI model from OpenAI spits out videos based on simple text prompts and we are talking**minute-long videos**that feel insanely real. Marques Brownlee (MKBHD) posted a[YouTube video](https://www.youtube.com/watch?v=NXpdyAWLDas\&list=WL\&index=3\&ab_channel=MarquesBrownlee\&utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)talking about it. So, what can you dream now, because Sora (sky in Japanese) is the limit.üçø[Our Summary](https://bensbites.beehiiv.com/p/ai-video-isnt-patchy-anymore-almost-real)(also below)

4. [Scribe just bagged $25M in Series B funding](https://scribe.how/bensbites-2?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything). They want to give you a well-deserved break from answering people‚Äôs questions all day. Instead, just create step-by-step guides (automatically, thanks to AI) to share with your team. (I‚Äôm an investor)

also noticed some chatter on Twitter about a new model named Mistral-Next. Crazzzy‚Ä¶

###### **TOP TOOLS**

- [Glif](https://twitter.com/fabianstelzer/status/1758148516503769553?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)-**Remix any image**on the web.

- [Magika by Google](https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)- AI-powered fast and efficient**file type identification.**

- [Squad](https://meetsquad.ai/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)- The**product strategy**tool that delivers for you and your users.

- [Persona by Diarupt](https://diarupt.ai/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)- Human-like**AI teammates.**

- [Lindy](https://www.lindy.ai/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)- Create and monetize custom AI agents in minutes.

- [HeyDay](https://heyday.xyz/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)- Turn your**information into insights**

- [Intent by Upflowy](https://www.upflowy.com/intent?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)- Turn your**leads' behaviour**into AI Summaries.

[View more ‚Üí](https://news.bensbites.co/tags/show?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)

###### **NEWS**

More launches

- **[Google quietly launches Goose](https://www.businessinsider.com/google-goose-ai-model-language-ai-coding-2024-2?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)**- An internal AI model to help employees write code faster.

- Apple is working on a\*\*[copilot for Xcode](https://www.bloomberg.com/news/articles/2024-02-15/apple-s-ai-plans-github-copilot-rival-for-developers-tool-for-testing-apps?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\*\*\*\*,\*\*it's software programming tool.

- **[V-Jepa by Meta](https://www.fastcompany.com/91029951/meta-v-jepa-yann-lecun?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)**- A new AI model that learns about the world by watching videos.

More money

- \*\*[Magic Dev raises $100M](https://twitter.com/danielgross/status/1758180171520028695?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\*\*for creating an AI software engineer.

- \*\*[Langchain has raised $20M](https://blog.langchain.dev/langsmith-ga/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)**series A and Langsmith is generally available now.**[Sequoia‚Äôs POV](https://www.sequoiacap.com/article/partnering-with-langchain-the-llm-application-framework/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)**on the investment and**[Forbes](https://www.forbes.com/sites/alexkonrad/2024/02/15/open-source-ai-startup-langchain-launches-langsmith/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\*\*coverage.

- \*\*[Guardrails AI has raised $7.5M](https://twitter.com/ShreyaR/status/1758175746563145733?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\*\*in seed funding for open-source AI reliability.

- GPU cloud provider\*\*[Lambda Labs confirms $320M](https://www.theinformation.com/briefings/gpu-cloud-provider-lambda-labs-confirms-320-million-in-new-funding?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\*\*in new funding.

and regular news

- **[ROBOTS.TXT](https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)**- The text file that ran the internet, does it now?

- The era of abstraction & new\*\*[creative tensions.](https://www.implications.com/p/the-era-of-abstraction-and-new-creative?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\*\*

- \*\*[Sam Altman owns](https://www.axios.com/2024/02/15/sam-altman-openai-startup-fund?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\*\*OpenAI's venture capital fund.

- \*\*[Instacart is using generative AI](https://www.businessinsider.com/retail/news/instacart-is-using-ai-art-its-incredibly-unappetizing-/articleshow/107210803.cms?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\*\*to create photography for its recipes.

[View more ‚Üí](https://news.bensbites.co/tags/news/trending?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)

###### **QUICK BITES**

[Google announced a new model.](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)Didn‚Äôt they release one last week? That was[Gemini Ultra 1.0](https://bensbites.beehiiv.com/p/google-launches-gemini-advanced-compete-openai). Google‚Äôs moving ahead to 1.5 model announcements and now we got a peek into Gemini Pro 1.5. This one has a context window of up to 10M tokens‚ÄîGPT-4 Turbo has 128k.

**What is going on here?**

Google introduced Gemini Pro 1.5 - A new model with insane context window and performance.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/65814b8a-24ad-42f2-8a41-fbf3dbf47662/image.png?t=1708084371)

**What does this mean?**

Google‚Äôs already on the treadmill after just giving people access to their best model last week and announced Gemini Pro 1.5 now. So what‚Äôs new here?

- It is based on the**Mixture of Experts**architecture. (which many people believe is the secret sauce behind GPT-4).

- Just like other Gemini models, it is**multimodal**from the ground up‚Äîunderstands images, video, and audio natively.

- This new model can have up to**10M tokens**in its context window. The big hype feature.

- Despite being a mid-sized model, it performs at a**similar level to Gemini Ultra 1.0**‚ÄîThe silent killer. Ultra 1.0 is Google‚Äôs biggest model and GPT-4 class model.

Let‚Äôs understand these a bit:

Context window means how long your prompt to an AI model can be and if you‚Äôre working with long-form content like business PDFs, books etc. you want all you can get. Claude by Anthropic shocked everyone by accepting 100k tokens in its context window last summer (200k some months later) and GPT-4 Turbo announced a 128k token context window in November.

Gemini Pro 1.5 takes two big jabs at other models here:

- Huge jump from what they call ‚Äústandard‚Äù i.e. 128k tokens. The 10M context window is a research claim but Google is allowing select developers to test up to 1M tokens.

- Multimodal inputs: You can not only put large books in there, but you can query entire movies, languages, codebases, and whatnot.

And these jabs land because in evaluation they have 100% recall till 500k-ish token and >99% till 10M. Examples[here.](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)

Twitter‚Äôs going gaga over this part but some amazing stuff is hiding underneath all this talk about context window. It‚Äôs killer performance on multiple benchmarks.

Google is[reporting the performance](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)\[page 20] with respect their their Gemini Ultra 1.0 model, not GPT-4 but cross-referencing technical reports, I found Pro 1.5 to be*very slightly*better than GPT-4‚Äôs on MATH, BIG-Bench-Hard and nearby on a bunch of others.

The trick seems to be a Mixture of Experts architecture. The leaks (although most researchers believe this) say that GPT-4 is also a ‚Äúmixture of experts‚Äù model. And Mistral is also using it in their models to punch above their weight. I‚Äôm excited to see what Google does with Gemini Ultra and MoE.

**Why should I care?**

I‚Äôll just remind you of the three examples from Google itself:

- Finding contextual quotes from 402-page transcripts from Apollo 11‚Äôs mission to the moon.

- Loading up a 44-minute silent movie and finding exact scenes.

- Programming with a reference of 100,000+ lines of code.

Long context means even if the AI doesn‚Äôt know stuff, you can just put the reference material and get your work done. If that‚Äôs ain‚Äôt exciting, I don‚Äôt know what is.

Wait. I know something. How about almost real-looking AI videos?

[*Share this story*](https://bensbites.beehiiv.com/p/gemini-pro-15-google-10m-content-window)

###### **QUICK BITES**

[OpenAI's mogging everyone in the AI space again.](https://openai.com/sora?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)Sora, a new AI model from OpenAI spits out videos based on simple text prompts and we are talking minute-long videos that feel insanely real. So what can you dream now, because Sora (sky in Japanese) is the limit.

**What is going on here?**

Open AI‚Äôs new text-to-video model, Sora, is a major leap for video-generating AI.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/c3ef7051-8ab9-496f-bbb1-16d17d23ba18/image.png?t=1708088325)

**What does this mean?**

Text-to-video models have been improving gradually since early last year. We started with Will Smith choking on Spaghetti and started getting scripted history, dragon worlds, and decentish-looking videos. But two problems remain:

- The videos are still janky. You can tell it‚Äôs AI.

- They aren‚Äôt much long 4 secs, 10 secs, 20 if you push it.

OpenAI broke the chain of gradual improvement and came in an entire level upgrade (or even many levels maybe) with Sora.

Sora‚Äôs videos are smooth, dynamic, consistent and go up to 1 minute. You can get detailed: the style of animation, mood, camera angles, etc. Imagine specifying ‚ÄúWes Anderson directs a Pixar short about hamsters." Sora aims to deliver.

Sora is not out for use yet. Everyone copy-pasting the demo examples OpenAI released and I can‚Äôt blame them, they are unrealüòâ.

But let‚Äôs dig a bit deeper into Sora‚Äôs[technical report](https://openai.com/research/video-generation-models-as-world-simulators?utm_source=bensbites\&utm_medium=referral\&utm_campaign=daily-digest-what-s-possible-everything)and see what OpenAI claims:

1. Sora can create videos in a large range of aspect ratios and resolutions. From widescreen 1920x1080p videos, vertical 1080x1920 videos and everything in between.

2. Similar to DALL¬∑E 3, OpenAI uses language models (GPT) to turn basic prompts into power prompts getting high-quality videos.

3. Sora can use images and videos as inputs, not just text. That means:

   - It can animate images.

   - It can extend videos: backwards and forwards.

   - It can edit videos like changing the scene with keeping characters the same.

   - It can connect two videos, filling the in-between frames automatically.

The wildest claim OpenAI has (and we can see the hints) is that Sora understands the world through videos. It understands 3D motion, behaviour of objects and complex interactions (not perfect though). This all leads to creating a model that simulates our world to the best extent we can.

But how is OpenAI‚Äôs model this good, across all this stuff? In their own words:\*\*they are purely phenomena of scale.\*\*The best explanation of that is this demo with base compute, 4x and 16x compute.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/43960dcc-8882-42bf-929c-cbb5fea1b397/image.png?t=1708088093)

**Why should I care?**

AI just got serious about filmmaking and it‚Äôs gonna kill the video industry.

Just kidding, we don‚Äôt cry wolf around here. But in all honesty, get ready for a wave of AI-generated video hitting the web. Sora (and upcoming models) will not only make long, visually consistent videos, but they‚Äôll also handle complex prompts with characters, emotions, and multiple scene changes.

Seeing is... not always believing. At least not anymore. It'll get harder to tell what's real footage and what's been cooked up by an AI. Limitations exist. Physics can be wacky in these videos, and Sora might misinterpret some directions. Don't throw out your special effects team just yet, but take note.

[*Share this story*](https://bensbites.beehiiv.com/p/ai-video-isnt-patchy-anymore-almost-real)

### Ben‚Äôs Bites Insights

We have 2 databases that are updated daily which you can access by sharing Ben‚Äôs Bites using the link below;

- **All 10k+ links**we‚Äôve covered, easily filterable (1 referral)

- **6k+ AI company funding rounds**from Jan 2022, including investors, amounts, stage etc (3 referrals)
