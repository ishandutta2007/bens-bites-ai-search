# Fuyu-8B from Adept is multimodal, and open source.

Wake up, babe! [Adept is open-sourcing Fuyu-8B.](https://www.adept.ai/blog/fuyu-8b?utm_source=bensbites\&utm_medium=referral\&utm_campaign=fuyu-8b-from-adept-is-multimodal-and-open-source) Fuyu (hip name btw) is multimodal, i.e. it can see pictures AND read text. The model is available on HuggingFace and is designed for digital agents to understand images and text.

## What's going on here?

The AI squad at Adept just dropped an open-source multimodal model called Fuyu-8B.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/c6738a4a-f8a5-4715-901d-e897104caac2/image.png)

## What does this mean?

Unlike other multimodal cuties, Fuyu-8B keeps it simple. She feeds images right into her transformer decoder so she can work with any image size. No separate image encoder or complex training. Fuyu-8B's chill with charts, diagrams, and docs—she answers questions about them like a boss.

On common benchmarks, Fuyu-8B outperforms models with more parameters, showing its efficient architecture. However, these benchmarks have issues, so Adept be like: *no worries, we’ll build our own.*

Fuyu-8B is a small version of the larger multimodal model that powers their products. Her big sis Fuyu-Medium does next-level stuff like OCR scanned docs and pinpointing UI elements. Adept is keeping their bigger models under wraps for now—fair.

## Why should I care?

An open multimodal model is a big step for AI. Simpler architecture = more accessible and scalable. Fuyu-8B is a solid base for researchers and devs to build real-world apps.

Understanding visual data matters for business. Precision OCR/localization unlocks assistants that can see screens like humans and take action. The Fuyu models are geared toward knowledge workers, worth checking the detailed examples on the blog if you’re one.
