# Open AI releases DALL-E 3

[Open AI has announced its next image model, DALL-E 3.](https://openai.com/dall-e-3?utm_source=bensbites\&utm_medium=referral\&utm_campaign=open-ai-releases-dall-e-3) DALL-E 3 understands significantly **more nuance and detail** than DALL-E 2. Achieving this nuance doesn’t require extensive prompt engineering with DALL-E 3. It is **built natively on ChatGPT**and Plus users would be able to use it similar to a ChatGPT plugin starting in October.

## What's going on here?

OpenAI’s latest image model, DALL-E 3 is now available in research preview.

![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/260da4e5-ac4e-4f86-9161-d7df196c80b3/image.png)

## What does this mean?

DALL-E 3 aims to solve the tendency of image models to ignore secondary details in the prompt. Integration with ChatGPT means that you can use OpenAI’s language models along with DALL-E 3 which allows more detailed brainstorming. The new model will also be available via API and Open AI labs soon.

## Why should I care?

ChatGPT lacks visual AI skills. The addition of DALL-E 3 to ChatGPT kills two birds with one stone: making the product better and taking on Google whose upcoming Gemini model is also multimodal (images and text together). API and Labs availability of this improved model will also attract developers who had been looking towards Midjourney and SDXL for their projects.
